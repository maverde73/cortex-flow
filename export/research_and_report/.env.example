# LLM Provider Configuration
# Uncomment and set the API keys for your preferred provider

# OpenAI
#OPENAI_API_KEY=your-openai-key-here
#OPENAI_MODEL=gpt-4o-mini

# Anthropic
#ANTHROPIC_API_KEY=your-anthropic-key-here
#ANTHROPIC_MODEL=claude-3-haiku-20240307

# Groq
#GROQ_API_KEY=your-groq-key-here
#GROQ_MODEL=mixtral-8x7b-32768

# OpenRouter
#OPENROUTER_API_KEY=your-openrouter-key-here
#OPENROUTER_MODEL=openai/gpt-3.5-turbo

# Ollama (local)
#OLLAMA_BASE_URL=http://localhost:11434
#OLLAMA_MODEL=llama2

# Server Configuration
MCP_HOST=0.0.0.0  # Host to bind to (0.0.0.0 for all interfaces, needed for Docker)
MCP_PORT=8000  # Port for HTTP transport (change if 8000 is already in use)
LLM_TIMEOUT=300  # Timeout in seconds for LLM calls (default 300 = 5 minutes)
WORKFLOW_TIMEOUT=600  # Overall workflow timeout in seconds (default 600 = 10 minutes)

# Logging
LOG_LEVEL=INFO
